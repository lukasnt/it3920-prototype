version: "3.9"

services:
  zeppelin:
    build:
      context: .
    ports:
      - "8080:8080"
      - "4040:4040"
    env_file:
      - .env
    profiles:
      - debug
    volumes:
      - type: bind
        source: ./zeppelin-notebooks
        target: /opt/zeppelin/notebook
        read_only: false
      - type: bind
        source: ./target
        target: /opt/zeppelin/target
        read_only: false
      - type: bind
        source: ./logs
        target: /opt/zeppelin/logs
        read_only: false
      - ./log4j.properties:/opt/zeppelin/spark/conf/log4j.properties
    mem_reservation: 12000M
    cpu_count: 8
    command: /opt/zeppelin/bin/zeppelin-daemon.sh upstart
    tty: true

  package-restart:
    depends_on:
      - zeppelin
    build:
      context: .
      target: mvn-package
    profiles:
      - debug
    ports:
      - "8081:8081"
    volumes:
      - type: bind
        source: ./target
        target: /usr/home/spark-graphx-scala/target
        read_only: false
      - ./src:/usr/home/spark-graphx-scala/src
      - ./pom.xml:/usr/home/spark-graphx-scala/pom.xml
    working_dir: /usr/home/spark-graphx-scala
    command: >
      bash -c "
      mvn package
      && curl -X PUT http://zeppelin:8080/api/interpreter/setting/restart/spark
      && curl -X POST http://zeppelin:8080/api/notebook/job/2HSV6W3AT
      "

  spark-submit:
    env_file:
      - .env
    build:
      context: .
      target: spark-v2_4_0
    profiles:
      - spark-submit
    volumes:
      - type: bind
        source: ./target
        target: /opt/spark/target
        read_only: false
      - ./log4j.properties:/opt/spark/conf/log4j.properties
    mem_reservation: 12000M
    cpu_count: 8
    entrypoint: /opt/spark/bin/spark-submit --class com.lukasnt.spark.App --deploy-mode client --master local --driver-memory 12g --driver-cores 16 --executor-memory 12g --executor-cores 16 --conf spark.executor.instances=2 --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties --conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties /opt/spark/target/spark-graphx-scala-1.0-SNAPSHOT-jar-with-dependencies.jar


  spark-history-server:
    build:
      context: .
      target: spark-v2_4_0
    profiles:
      - debug
    ports:
      - "18080:18080"
    env_file:
      - .env
    volumes:
      - type: bind
        source: ./logs/spark-events
        target: /opt/spark/logs
        read_only: false
    command: >
      bash -c "
      /opt/spark/sbin/start-history-server.sh 
      && tail -F anything
      "

  hdfs-namenode-reformat:
    build:
      context: .
      target: hadoop
    profiles:
      - hdfs-reformat
    volumes:
      - type: bind
        source: ./hdfs/namenode
        target: /opt/hdfs/namenode
        read_only: false
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    command: >
      bash -c "
      /opt/hadoop/sbin/stop-dfs.sh
      && yes | hdfs namenode -format
      "

  hdfs-namenode-master:
    build:
      context: .
      target: hadoop
    ports:
      - "50070:50070"
      - "9870:9870"
    profiles:
      - hdfs
    env_file:
      - .env
    volumes:
      - type: bind
        source: ./hdfs/namenode
        target: /opt/hdfs/namenode
        read_only: false
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    command: >
      bash -c "
      /opt/hadoop/sbin/stop-dfs.sh
      && hdfs namenode
      "

  hdfs-datanode:
    depends_on:
      - hdfs-namenode-master
    build:
      context: .
      target: hadoop
    ports:
      - "50010:50010"
      - "9864:9864"
    profiles:
      - hdfs
    env_file:
      - .env
    volumes:
      - type: bind
        source: ./hdfs/datanode
        target: /opt/hdfs/datanode
        read_only: false
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    command: >
      bash -c "
      /opt/hadoop/sbin/stop-dfs.sh
      && hdfs datanode
      "

  ldbc-datagen:
    build:
      context: .
      target: ldbc-snb-datagen
    profiles:
      - ldbc-datagen
    env_file:
      - .env
    volumes:
      - type: bind
        source: ./datasets
        target: /opt/ldbc-snb-data
        read_only: false
    command: >
      bash -c "
      mkdir -p OUTPUT_DIR
      && /ldbc_snb_datagen_spark/tools/run.py --parallelism 8 --memory 12g -- --format csv --scale-factor $SCALE_FACTOR --mode raw --output-dir $OUTPUT_DIR
      "