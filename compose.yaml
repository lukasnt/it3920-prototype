services:
  zeppelin:
    build:
      context: .
    ports:
      - "8080:8080"
      - "4040:4040"
    profiles:
      - debug
    volumes:
      - type: bind
        source: ./zeppelin-notebooks
        target: /opt/zeppelin/notebook
        read_only: false
      - type: bind
        source: ./target
        target: /opt/zeppelin/target
        read_only: false
      - type: bind
        source: ./logs
        target: /opt/zeppelin/logs
        read_only: false
      - ./log4j.properties:/opt/zeppelin/spark/conf/log4j.properties
    environment:
      - ZEPPELIN_NOTEBOOK_DIR=/opt/zeppelin/notebook
      - ZEPPELIN_ADDR=0.0.0.0
      - ZEPPELIN_PORT=8080
      - ZEPPELIN_MEM=-Xmx16384m
    command: /opt/zeppelin/bin/zeppelin-daemon.sh upstart
    tty: true

  package-restart:
    depends_on:
      - zeppelin
    build:
      context: .
      target: mvn-package
    profiles:
      - debug
    ports:
      - "8081:8081"
    volumes:
      - type: bind
        source: ./target
        target: /usr/home/spark-graphx-scala/target
        read_only: false
      - ./src:/usr/home/spark-graphx-scala/src
      - ./pom.xml:/usr/home/spark-graphx-scala/pom.xml
    working_dir: /usr/home/spark-graphx-scala
    command: >
      bash -c "
      mvn package
      && curl -X PUT http://zeppelin:8080/api/interpreter/setting/restart/spark
      && curl -X POST http://zeppelin:8080/api/notebook/job/2HSV6W3AT
      "

  spark-history-server:
    build:
      context: .
      target: spark-v2_4_0
    profiles:
      - debug
    ports:
      - "18080:18080"
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:/opt/spark/logs -Dspark.history.ui.port=18080
    volumes:
      - type: bind
        source: ./logs/spark-events
        target: /opt/spark/logs
        read_only: false
    command: >
      bash -c "
      /opt/spark/sbin/start-history-server.sh 
      && tail -F anything
      "

  hdfs-namenode-reformat:
    build:
      context: .
      target: hadoop
    profiles:
      - hdfs-reformat
    volumes:
      - type: bind
        source: ./hdfs/namenode
        target: /opt/hdfs/namenode
        read_only: false
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    command: >
      bash -c "
      /opt/hadoop/sbin/stop-dfs.sh
      && yes | hdfs namenode -format
      "

  hdfs-namenode-master:
    build:
      context: .
      target: hadoop
    ports:
      - "50070:50070"
    profiles:
      - hdfs
    environment:
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
      - HDFS_SECONDARYNAMENODE_USER=root
      - JAVA_HOME=/usr/lib/jvm/java-8-openjdk
    volumes:
      - type: bind
        source: ./hdfs/namenode
        target: /opt/hdfs/namenode
        read_only: false
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    command: >
      bash -c "
      /opt/hadoop/sbin/stop-dfs.sh
      && hdfs namenode
      "

  hdfs-datanode:
    depends_on:
      - hdfs-namenode-master
    build:
      context: .
      target: hadoop
    ports:
      - "50010:50010"
    profiles:
      - hdfs
    environment:
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
      - HDFS_SECONDARYNAMENODE_USER=root
    volumes:
      - type: bind
        source: ./hdfs/datanode
        target: /opt/hdfs/datanode
        read_only: false
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    command: >
      bash -c "
      /opt/hadoop/sbin/stop-dfs.sh
      && hdfs datanode
      "